# CacheNT #

---


This page discusses the intended New Technology (groan) of the caching infrastructure of route-me. The goal is to replace the existing redundant caching system which has high overhead and some bug issues, with a lean, high performance, multi-threaded caching infrastructure.



## Thread Structure Overview ##

---


**Main Thread**

Tile request generated to the RMTileFactory

tile factory = { 1. RMPrimaryTileCache, 2. covers access to RMSecondaryTileCache }

**Thread Boundary**

RMSecondaryTileCache -> { 1. storage, 2. network }


**Communication**

Communication between tile request initiator (client) and callback from the secondary cache / network fetch is TBD. Candidates:

1. Standard delegation: Tile Factory will receive a message from the secondary cache thread, and the initiate a delegate message to the client.

2. Notification: Notification will be generated by calling the NSNotificationCenter in the main thread from the secondary cache thread, this will update both the RAM Cache and the client.

3. KVO: Currently not being considered. If there is a strong argument for this in light of the others can consider it.

The goal is to provide the lightest weight and fastest implementation, and the appropriate choice will be made. If delegation can be done without causing a big mess, this will be the route.

So I will maintain flexibility in this regard.

## Tile Factory Interface ##

---


Example requests _may_ look like this:

```
RMTileImage *tile = [RMTileFactory imageForTile:(RMTile)tile delegate:someDelegate];
```

It is possible that I will put the RMTileFactory functionality into RMTileImage as class methods for generating RMTileImages. The benefit is that there is one less class to muddy things up and it will follow the same overall architecture of UIImage, where UIImage is given image names to load images, performs caching, etc., we will give RMTileImage a name (tile) delegate for asynchronous callback, and possibly the tile source. This will allow multiple tile sources to be used, and in some situations this may be necessary if not a feature that some people may wish to use.

Summary:

  1. if you want to receive tiles you register for a notification for tiles (see below note about communication)
  1. you query the tile factory directly for a tile, it will either return you a neutral-ref-count tile, or nil
  1. if you receive a tile, you go about your merry way, if you do not, you go away and sulk and wait
  1. if you did not receive a tile in (3) a response will be generated with your tile in it, receive and do what you want

**Tile Factory Implementation**

Tile factory holds access to a RAM cache. RAM Cache is tested for presence of the tile. If there, handed right back to the caller. By avoiding a notification in this step, we save allocation of notification resources, we save a trip back to the main event loop, production of a new event and step down through the event handling system to notification production and then handing over the tile. By doing it this way, we get the tile to the screen that much faster, with fewer resources used. Less space + less time = win by default. We want this in the main thread because we want to not have to go over the thread boundary if we can easily avoid it.

If the tile is not in the cache, then we are best off letting the caller return empty handed so the application can return to work, so in this case it will come back via notification to be as flexible as possible.

The tile factory will then signal the secondary cache thread that a tile is requested. At this point the factory is done and washes its hands of the request.

**Primary Cache Implementation**

This will be implemented in C using a core foundation heap. This will provide O(1) access to the LRU item to be popped and disposed of, but O(log(n)) insert and delete. The current implementation is an array which is O(n) for delete and O(1) insert. Overall the heap will provide better performance and will also scale much, much better with larger data sets. The C implementation will also run circles around the ObjC implementation, though the cache is not very frequently hit, there is no choice here since there is no NSHeap in the Foundation Kit.

**Secondary Cache Implementation**

The secondary cache checks secondary storage and extracts if the tile is there. If it is not, it initiates a network request and then goes back to monitoring network data, writing anything to storage that needs writing, and new requests. If a response arrives from the network, it generates a call on the main thread that a tile is ready and provides the tile.

## URL Caching ##

---


The kit is currently caching tiles in the URL Cache (which is RAM only), a RAM cache implemented with an array, and SQLite. Requests in the future will indicate to the UIKit that URL caching is not to be allowed, as we will handle responsibility for RAM caching directly. SQLite, see below.

This:

```
[[NSURLCache sharedURLCache] setMemoryCapacity: 0];
```

May be indicated to application developers as a means of further reducing memory overhead, but for our purposes, it looks like it will override the application developer's ability to cache URLs should they be handling other requests.

It is possible to add this as a defaults setting, set to NO initially, something of the line kRMDisableGlobalURLCache with its value as kRMDefaultDisableGlobalURLCache which will be defined to NO. This will be stored in NSUserDefaults, so can be overridden in numerous ways at the choice of the application developer, and we can provide a class method interface to it as well that will set and get the settings values and store the results in the defaults database as backing store.


## Data Storage ##

---


Currently the kit is using SQLite to store tiles. There is currently a 4M RAM overhead for using SQLite. It is not clear at the moment if this is a feature or a bug, it just is. SQLite seems like overkill for our requirement which is hashing a key to an image. It is considered that storing images in the filesystem, hashing the key and returning the image will be the fastest and lowest overhead method of returning an image to the higher level of the kit. It will also allow for an app-cache to ship under the .app directory for people who want to distribute tiles with their application. The desire is to continue to support SQLite for people who Really Must Have It, and this will probably be done by abstracting the secondary storage cache through a superclass, and having different implementation subclasses.

**Object Storage**

The secondary storage cache will be written only with the concepts that it is basically a URL to data cache. It will not know or care anything in specific about what it is storing. This will preserve some flexibility for the future should we have some need or desire to place other things in the cache.

**Overview**

Tile servers out there have aliases.

```
(a,b,c).tilesource.provider.com
```

May point to different machines or the same machine. They set it up like this so you can multiplex
downloads and get past browsers limiting your access. Right now, in the kit, the tile servers are
usually listed directly as one host:

```
a.tilesource.provider.com
```

For instance.

Change #1 here has to allow for multiplexing of downloads, we can speed up network access
by accessing the parallel hosts. So into the bucket goes a way for the tile source classes to
identify aliased hosts, and how many should be accessed in parallel to download tiles.

Secondary storage structure:

![http://hostname/path/tile.png](http://hostname/path/tile.png)

The path component is going to be unique on the host, but the hostnames may be aliases within
a provider, all pointing to the same resource, and the paths are not guaranteed unique provider
to provider.

So this implies a structure:

```
/{App_ID}/Caches/tiledb/{Source_Identifier}/path/tile.png
```

We can assign the Source\_Identifier using the class name for the tile source, which also has
to be unique in the kit. (a,b,c).tilehost.maphost.com will then all map to Provider\_Identifier,
and the secondary storage cache will then multiplex downloads according to how many the
application programmer has specified he wants fetched in parallel.

Reads and writes will be made directly to the filesystem.



**Tile Source / Relative Tile**

A Tile Provider is uniquely identified by {host, path\_format}

A Tile Source is uniquely identified by {Tile\_Provider, style}

A Relative Tile is identified by {zoom, x position, y position}

A Tile is uniquely identified by {Tile\_Source, Relative\_Tile}

The Tile Provider is responsible for remapping x and y positions if necessary to match provider's representation types.




## Configuration ##

---


Caches are currently configured externally through a PList and there is no internal configuration. This will be changed to use the NSUserDefaults, with a published list of keys and registered defaults. This will allow the programmer to at application launch time reserve the decision to configure via his own code, by PList, application set defaults through application configuration panels, etc.

## Expiration ##

---


Expiration notes:

"Virtual Earth provides traffic overlays. Those should be cached for "a while", but then should be forced to expire. I can envision other situations where a tile source would produce time-sensitive data. So the cache certainly needs per-server expiration: "nothing downloaded more than X minutes ago", "nothing older than time T". It might even need per-tile expiration. You'll want to tag the tile's time of receipt, or maybe use a "time" field that can be either time of receipt or time of creation on server. If I'm downloading the once-a-day weather map 22 hours after it was created, I want to tag it with creation time, not download time, so that I'm forced to get a new one as soon as it's ready."

## Pre-fetch ##

---


The secondary cache will attempt at low priority to make some intelligent decisions about pre-fetching tiles based on current activity. This is going to be a low priority feature for now as it will depend on the rest working and implemented, but the utility of it will be high when done.